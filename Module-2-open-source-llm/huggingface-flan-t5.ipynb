{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d658f909-e679-41e9-9c4e-e0241c719049",
   "metadata": {},
   "source": [
    "## Quick Recap of Module 1\n",
    "\n",
    "If you're not running in Saturn Cloud, you need to install these libraries:\n",
    "\n",
    "Make sure you use the latest versions\n",
    "\n",
    "```\n",
    "pip install -U transformers accelerate bitsandbytes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506fab2a-a50c-42bd-a106-c83a9d2828ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:01.556021Z",
     "iopub.status.busy": "2024-07-08T14:45:01.555780Z",
     "iopub.status.idle": "2024-07-08T14:45:02.858558Z",
     "shell.execute_reply": "2024-07-08T14:45:02.858010Z",
     "shell.execute_reply.started": "2024-07-08T14:45:01.555999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-07-08 14:45:02--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3832 (3.7K) [text/plain]\n",
      "Saving to: ‘minsearch.py’\n",
      "\n",
      "minsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-07-08 14:45:02 (65.5 MB/s) - ‘minsearch.py’ saved [3832/3832]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!rm -f minsearch.py\n",
    "!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33f625c-bb7e-4176-a500-93bc283b8c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:02.861246Z",
     "iopub.status.busy": "2024-07-08T14:45:02.860678Z",
     "iopub.status.idle": "2024-07-08T14:45:02.865428Z",
     "shell.execute_reply": "2024-07-08T14:45:02.864620Z",
     "shell.execute_reply.started": "2024-07-08T14:45:02.861211Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['HF_HOME'] = '/run/cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac947de-effd-4b61-8792-a6d7a133f347",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:02.867045Z",
     "iopub.status.busy": "2024-07-08T14:45:02.866480Z",
     "iopub.status.idle": "2024-07-08T14:45:04.150897Z",
     "shell.execute_reply": "2024-07-08T14:45:04.150157Z",
     "shell.execute_reply.started": "2024-07-08T14:45:02.867012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.Index at 0x7f81e868f760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests \n",
    "import minsearch\n",
    "\n",
    "docs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)\n",
    "\n",
    "index = minsearch.Index(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f087272-b44d-4738-9ea2-175ec63a058b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:04.152935Z",
     "iopub.status.busy": "2024-07-08T14:45:04.152243Z",
     "iopub.status.idle": "2024-07-08T14:45:04.157078Z",
     "shell.execute_reply": "2024-07-08T14:45:04.156430Z",
     "shell.execute_reply.started": "2024-07-08T14:45:04.152898Z"
    }
   },
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742ab881-499a-4675-83c4-2013ea1377b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:04.158758Z",
     "iopub.status.busy": "2024-07-08T14:45:04.158219Z",
     "iopub.status.idle": "2024-07-08T14:45:04.162919Z",
     "shell.execute_reply": "2024-07-08T14:45:04.162350Z",
     "shell.execute_reply.started": "2024-07-08T14:45:04.158726Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7616ba02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:04.163990Z",
     "iopub.status.busy": "2024-07-08T14:45:04.163708Z",
     "iopub.status.idle": "2024-07-08T14:45:04.545040Z",
     "shell.execute_reply": "2024-07-08T14:45:04.544429Z",
     "shell.execute_reply.started": "2024-07-08T14:45:04.163971Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57ebc096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:04.547216Z",
     "iopub.status.busy": "2024-07-08T14:45:04.546877Z",
     "iopub.status.idle": "2024-07-08T14:45:04.551617Z",
     "shell.execute_reply": "2024-07-08T14:45:04.551048Z",
     "shell.execute_reply.started": "2024-07-08T14:45:04.547193Z"
    }
   },
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe8bff3e-b672-42be-866b-f2d9bb217106",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:04.552789Z",
     "iopub.status.busy": "2024-07-08T14:45:04.552503Z",
     "iopub.status.idle": "2024-07-08T14:45:04.556860Z",
     "shell.execute_reply": "2024-07-08T14:45:04.556087Z",
     "shell.execute_reply.started": "2024-07-08T14:45:04.552767Z"
    }
   },
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae83054",
   "metadata": {},
   "source": [
    "## Running the FLAN-T5-XL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "091a77e6-936b-448e-a04b-bad1001f5bb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:04.557999Z",
     "iopub.status.busy": "2024-07-08T14:45:04.557753Z",
     "iopub.status.idle": "2024-07-08T14:45:10.916618Z",
     "shell.execute_reply": "2024-07-08T14:45:10.916054Z",
     "shell.execute_reply.started": "2024-07-08T14:45:04.557977Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d6acf34f904a8f82b60e0a16168db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pip install accelerate\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\",\n",
    "                                                   device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21aa255e-c971-44ca-9826-a721df3ad063",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:10.918044Z",
     "iopub.status.busy": "2024-07-08T14:45:10.917637Z",
     "iopub.status.idle": "2024-07-08T14:45:10.922143Z",
     "shell.execute_reply": "2024-07-08T14:45:10.921396Z",
     "shell.execute_reply.started": "2024-07-08T14:45:10.918019Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"translate English to French: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c784114-b16d-4ff0-b22f-e192b62bad79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:10.924001Z",
     "iopub.status.busy": "2024-07-08T14:45:10.923366Z",
     "iopub.status.idle": "2024-07-08T14:45:10.929683Z",
     "shell.execute_reply": "2024-07-08T14:45:10.928864Z",
     "shell.execute_reply.started": "2024-07-08T14:45:10.923968Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13959,  1566,    12,  2379,    10,   571,   625,    33,    25,    58,\n",
       "             1]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5e7a3c2-0335-4c96-bfa5-63e5b222a442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:10.931130Z",
     "iopub.status.busy": "2024-07-08T14:45:10.930606Z",
     "iopub.status.idle": "2024-07-08T14:45:12.094570Z",
     "shell.execute_reply": "2024-07-08T14:45:12.093702Z",
     "shell.execute_reply.started": "2024-07-08T14:45:10.931109Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 5257,    3, 6738,   18, 3249,   58,    1]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model.generate(input_ids)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba54fdd6-04a4-4997-8350-e8b8af08a899",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:12.096379Z",
     "iopub.status.busy": "2024-07-08T14:45:12.095761Z",
     "iopub.status.idle": "2024-07-08T14:45:12.101546Z",
     "shell.execute_reply": "2024-07-08T14:45:12.100784Z",
     "shell.execute_reply.started": "2024-07-08T14:45:12.096340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> Comment êtes-vous?</s>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54893f0b-4016-45c1-870d-b879d341a765",
   "metadata": {},
   "source": [
    "## Modifying the LLM function to include our FLAN-T5-XL model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4903da63-4671-4265-85fb-76a5086e1c20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:12.102971Z",
     "iopub.status.busy": "2024-07-08T14:45:12.102634Z",
     "iopub.status.idle": "2024-07-08T14:45:12.108721Z",
     "shell.execute_reply": "2024-07-08T14:45:12.107783Z",
     "shell.execute_reply.started": "2024-07-08T14:45:12.102947Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt\n",
    "\n",
    "def llm(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, )\n",
    "    result = tokenizer.decode(outputs[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8804881f-16c2-4c0f-908e-17aa9ab18805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:12.110563Z",
     "iopub.status.busy": "2024-07-08T14:45:12.110043Z",
     "iopub.status.idle": "2024-07-08T14:45:12.116931Z",
     "shell.execute_reply": "2024-07-08T14:45:12.116078Z",
     "shell.execute_reply.started": "2024-07-08T14:45:12.110529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llm(prompt, generate_params=None):\n",
    "    if generate_params is None:\n",
    "        generate_params = {}\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=generate_params.get(\"max_length\", 100),\n",
    "        num_beams=generate_params.get(\"num_beams\", 5),\n",
    "        do_sample=generate_params.get(\"do_sample\", False),\n",
    "        temperature=generate_params.get(\"temperature\", 1.0),\n",
    "        top_k=generate_params.get(\"top_k\", 50),\n",
    "        top_p=generate_params.get(\"top_p\", 0.95),\n",
    "    )\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d73216d-70c5-4628-849a-80ab5c4a3be0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T14:45:12.118080Z",
     "iopub.status.busy": "2024-07-08T14:45:12.117788Z",
     "iopub.status.idle": "2024-07-08T14:45:15.724393Z",
     "shell.execute_reply": "2024-07-08T14:45:15.723632Z",
     "shell.execute_reply.started": "2024-07-08T14:45:12.118059Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(\"I just discovered the course. Can I still join it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6bafb-e2f1-405f-8cc0-0ea7a292888c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
