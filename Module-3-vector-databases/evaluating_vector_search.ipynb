{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T06:56:56.317148Z",
     "iopub.status.busy": "2024-10-05T06:56:56.316294Z",
     "iopub.status.idle": "2024-10-05T06:56:58.709054Z",
     "shell.execute_reply": "2024-10-05T06:56:58.708353Z",
     "shell.execute_reply.started": "2024-10-05T06:56:56.317107Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm.autonotebook import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading - `documents_with_ids.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start by loading our documents ids file\n",
    "with open(\"documents_with_ids.json\", \"rb\") as file:\n",
    "    documents = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'course': 'data-engineering-zoomcamp',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - When will the course start?',\n",
       " 'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       " 'id': '7000acaa'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising and Populating ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to ElasticSearch!\n"
     ]
    }
   ],
   "source": [
    "# initialising elasticsearch and checking to see if connection was successful\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "if es_client.ping():\n",
    "    print(\"Connected to ElasticSearch!\")\n",
    "else:\n",
    "    print(\"Connection Failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'course-questions'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now to create a new index as well as defining the index settings\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"question_vector\" :{\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            },\n",
    "            \"text_vector\":{\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\", # there is also another option of using dotp product which should yield the same result\n",
    "            },\n",
    "            \"question_text_vector\":{\n",
    "                \"type\": \"dense_vector\",\n",
    "                \"dims\": 384,\n",
    "                \"index\": True,\n",
    "                \"similarity\": \"cosine\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "# lets delete and create a new index if it exists for ease of re-runs\n",
    "if es_client.indices.exists(index=\"course-questions\"):\n",
    "    es_client.indices.delete(index=\"course-questions\")\n",
    "\n",
    "es_client.indices.create(index=index_name, body=index_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined the schema of our index, we can proceed create the vector embeddings before populating it\n",
    "\n",
    "# but first we need to load a pretrained sentence transformer model that would convert our questions and text to vector embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "\n",
    "# next we want to update our documents variable to include our vector embeddings while retaining the original data\n",
    "documents = [doc.update({\"question_vector\": model.encode(doc[\"question\"]),\n",
    "                         \"text_vector\": model.encode(doc[\"text\"]),\n",
    "                         \"question_text_vector\": model.encode(doc[\"question\"] + '-' + doc[\"text\"])}) or doc for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course-questions\n"
     ]
    }
   ],
   "source": [
    "# now to see if the index was indeed created\n",
    "\n",
    "indices = es_client.indices.get_alias(index=\"*\")\n",
    "\n",
    "for index in indices:\n",
    "    print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we simply have to populate our index using the bulk method\n",
    "# the bulk method is a `pipeline` where you can perform multiple actions in single request \n",
    "# for populating the index we only use the `index` action but there are others (e.g. delete, update or create)\n",
    "index = {'index':{\n",
    "    '_index': index_name}\n",
    "    }\n",
    "\n",
    "operations = [item for doc in documents for item in (index, doc)]\n",
    "\n",
    "response = es_client.bulk(operations=operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step for this function would be to define the search query function for elasticsearch\n",
    "def elastic_search_knn(field, vector, course):\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"filter\": {\n",
    "            \"term\": {\n",
    "                \"course\": course\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"text\", \"section\", \"question\", \"course\", \"id\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "\n",
    "    result_docs = [hit['_source'] for hit in es_results['hits']['hits']]\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating `elasticsearch` search engine using GTD - Hit Rate and MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-05T06:59:18.753736Z",
     "iopub.status.busy": "2024-10-05T06:59:18.753327Z",
     "iopub.status.idle": "2024-10-05T06:59:18.787299Z",
     "shell.execute_reply": "2024-10-05T06:59:18.786546Z",
     "shell.execute_reply.started": "2024-10-05T06:59:18.753707Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Course</th>\n",
       "      <th>document_ID</th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>7000acaa</td>\n",
       "      <td>When will the course start?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>7000acaa</td>\n",
       "      <td>What is the purpose of this document?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>7000acaa</td>\n",
       "      <td>How can I subscribe to the course public Googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>7000acaa</td>\n",
       "      <td>How can I register before the course starts?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data-engineering-zoomcamp</td>\n",
       "      <td>7000acaa</td>\n",
       "      <td>How can I join the course Telegram channel?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Course document_ID  \\\n",
       "0  data-engineering-zoomcamp    7000acaa   \n",
       "1  data-engineering-zoomcamp    7000acaa   \n",
       "2  data-engineering-zoomcamp    7000acaa   \n",
       "3  data-engineering-zoomcamp    7000acaa   \n",
       "4  data-engineering-zoomcamp    7000acaa   \n",
       "\n",
       "                                            Question  \n",
       "0                        When will the course start?  \n",
       "1              What is the purpose of this document?  \n",
       "2  How can I subscribe to the course public Googl...  \n",
       "3       How can I register before the course starts?  \n",
       "4        How can I join the course Telegram channel?  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the ground truth dataset\n",
    "ground_truth_df = pd.read_csv(\"ground-truth-data.csv\", delimiter=',', engine='python')\n",
    "\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is 1 null row that needs to be removed\n",
    "ground_truth_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now that we have loaded our ground truth dataset, we can evaluate our retrieval system\n",
    "# lets proceed to define a funtion that returns the first metric, hit rate\n",
    "# hit rate is the percentage of successful results where the system returns at least one relevant document\n",
    "\n",
    "def hit_rate_elasticserach(index_field:str, course:str, doc_id:str, query:str):\n",
    "    query_res = elastic_search_knn(index_field, vector=model.encode(query), course=course)\n",
    "\n",
    "    hit_rate = [True if res['id'] == doc_id else False for res in query_res]\n",
    "\n",
    "    if any(hit_rate):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to define a function that returns the rank of the relevant results for a particular query\n",
    "def mrr_elasticsearch(index_field:str,course:str, doc_id:str, query:str) -> int:\n",
    "    query_res = elastic_search_knn(index_field, vector=model.encode(query), course=course)\n",
    "\n",
    "    id_res = [res['id'] for res in query_res]\n",
    "\n",
    "    for index, id in enumerate(id_res):\n",
    "        if id == doc_id:\n",
    "            return 1/(index+1)\n",
    "            \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(index_field, df):\n",
    "    # calculating hit rate\n",
    "    df[f'hit_rate_{index_field}'] = df.apply(lambda x : hit_rate_elasticserach(index_field,x['Course'],x['document_ID'],x['Question']), axis=1)\n",
    "    hit_rate = df[f'hit_rate_{index_field}'].sum() / df[f'hit_rate_{index_field}'].count()\n",
    "\n",
    "    # calculating mrr\n",
    "    df[f'{index_field}_recip_rank'] = df.apply(lambda x : mrr_elasticsearch(index_field,x['Course'],x['document_ID'],x['Question']), axis=1)\n",
    "    mrr = df[f'{index_field}_recip_rank'].sum() / df[f'{index_field}_recip_rank'].count() \n",
    "\n",
    "    # we want to drop these columns that we created\n",
    "    df.drop(columns=[f\"hit_rate_{index_field}\", f\"{index_field}_recip_rank\"], inplace=True)\n",
    "\n",
    "    return {\n",
    "        f'hit rate for {index_field}' : hit_rate,\n",
    "        f'mrr for {index_field}' : mrr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hit rate for question_vector': 0.7293868921775899, 'mrr for question_vector': 0.6268745595489782}\n",
      "{'hit rate for text_vector': 0.779492600422833, 'mrr for text_vector': 0.6500211416490486}\n",
      "{'hit rate for question_text_vector': 0.8674418604651163, 'mrr for question_text_vector': 0.7651832276250881}\n"
     ]
    }
   ],
   "source": [
    "for index_field in [\"question_vector\",\"text_vector\",\"question_text_vector\"]:\n",
    "    print(evaluate(index_field, ground_truth_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saturn (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
