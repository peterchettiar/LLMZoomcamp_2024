# Week 4 notes

### Table of contents

- [4.1 Introduction to monitoring answer quality](#41-introduction-to-monitoring-answer-quality)
  - [Monitoring answer quality of LLMs](#-monitoring-answer-quality-of-llms)

## 4.1 Introduction to monitoring answer quality

Monitoring Large Language Model (LLM) systems is critical for ensuring they operate reliably, ethically and efficiently after they are deployed in a production environment.

Following is a high-level summary of the importance for monitoring said systems:

| **Category**                          | **Purpose**                                                                          | **Description**                                                                                         |
|---------------------------------------|--------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| **Performance Monitoring**            | Ensure model accuracy, relevance, latency, and scalability                            | Tracks model performance, response times, and load handling.                                             |
| **Resource Utilization**              | Manage computational and memory resources efficiently                                | Monitors CPU, memory, and storage to prevent resource exhaustion and optimize costs.                     |
| **Error Detection**                   | Identify and prevent model failures and anomalies                                     | Detects model crashes, misconfigurations, and abnormal behavior early on.                                |
| **Ethical and Safety Concerns**       | Mitigate bias, toxicity, and hallucinations in model responses                        | Ensures ethical behavior by tracking harmful, biased, or inaccurate outputs.                             |
| **User Behavior and Feedback**        | Improve model based on user interactions and feedback                                 | Analyzes user queries and feedback to fine-tune the modelâ€™s performance.                                 |
| **Security and Compliance**           | Maintain data privacy, model security, and regulatory compliance                      | Ensures adherence to data handling regulations and detects potential attacks.                            |
| **Model Drift and Degradation**       | Detect and address model performance decline over time                                | Monitors for performance degradation and allows for retraining when necessary.                           |
| **A/B Testing and Experimentation**   | Track performance of model variants to optimize improvements                          | Measures the effectiveness of different model versions during experimentation.                          |
| **Ethical AI Auditing**               | Provide transparency and accountability for model decisions                           | Enables tracking of interactions for audits, ensuring explainability and ethical usage.                  |
| **Legal and Regulatory Compliance**   | Ensure adherence to laws, regulations, and industry standards                         | Verifies compliance with legal requirements, especially in sensitive applications like healthcare.        |

This table highlights the main areas where monitoring LLM systems is crucial, with each category focusing on improving efficiency, ensuring security, and optimizing user satisfaction.

### Monitoring answer quality of LLMs

There is not one single metric that can be computed and it illustrates the overall performance of the LLM system or the RAG. There are many different set of metrics that can tell you if your model is doing well or not. Following are three example metrics used in the course;
1. Vector similarity between expected and LLM answer - this is very similar to what we did in the previous section using ground truth datasets. You simply evaluate the distance between the expected answer in the ground truth dataset against the answer generated by the LLM with respect to their individual vector embeddings.
2. LLM-as-a-judge to compute a toxicity of the LLM response - using another LLM model, say from huggingface, to determine the toxicity of the responses from our current model. Toxicity refers to a score that is defined by identifying the proportion of toxic opinions from to total number of opinions. The `ToxicityMetric` first uses an LLM to extract all opinions found in the `actual_output`, before using the same LLM to classify whether each opinion is toxic or not. You can learn more about this metric [here](https://docs.confident-ai.com/docs/metrics-toxicity).
3. LLM-as-a-judge to assess quality of LLM answer - 